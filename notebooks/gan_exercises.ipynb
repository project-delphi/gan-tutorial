{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Theoretical Exercises\n",
    "\n",
    "Following the original paper from [Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661), here are some exercises to help you understand the theory behind GANs.\n",
    "\n",
    "1. **Show that the optimal discriminator is** $D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$.\n",
    "\n",
    "2. **Show that the GAN objective reduces to** $V(G, D^*) = 2 \\cdot JS(p_{data} || p_g) - 2 \\cdot \\log 2$.\n",
    "\n",
    "3. **Show that the global minimum of the GAN objective is achieved if and only if** $p_g = p_{data}$.\n",
    "\n",
    "4. **Show that the global minimum of the GAN objective is** $- \\log 4$.\n",
    "\n",
    "5. **Show that the global minimum of the GAN objective is achieved if and only if** $D(x) = \\frac{1}{2}$.\n",
    "\n",
    "6. **Show that the global minimum of the GAN objective is achieved if and only if** $D(x) = D^*(x)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Here is the proof using $D(R)$ and $G(Z)$ explicitly, formatted for easy copying and pasting in markdown:\n",
    "\n",
    "Proof: The Optimal Discriminator in a GAN\n",
    "\n",
    "The goal is to find the optimal discriminator $D^*$ that maximizes the value function $V(G, D)$, defined as:\n",
    "\n",
    "$$\n",
    "V(G, D) = \\mathbb{E}_{R(x)}[\\log D(R(x))] + \\mathbb{E}_{Z}[\\log(1 - D(G(Z)))].\n",
    "$$\n",
    "\n",
    "The value function isn't strictly necessary for GAN training, we can just use the discriminator loss, but it's useful for understanding the theory. It helps set things up as a minimax game between the generator and discriminator.\n",
    "\n",
    "\n",
    "To find the optimal discriminator, we need to maximize $V(G, D)$ with respect to $D$. We can do this by taking the derivative of $V(G, D)$ with respect to $D$ and setting it to zero:\n",
    "\n",
    "First let's write out the integral form of the expectation:\n",
    "\n",
    "$$\n",
    "V(G, D) = \\int p_R(x) \\log D(R(x)) dx + \\int p_z(z) \\log(1 - D(G(z))) dz.\n",
    "$$\n",
    "\n",
    "Then we take the derivative with respect to $D$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial V(G, D)}{\\partial D} = \\int p_R(x) \\frac{1}{D(R(x))} \\frac{\\partial D(R(x))}{\\partial D} dx + \\int p_z(z) \\frac{1}{1 - D(G(z))} \\frac{\\partial (1 - D(G(z)))}{\\partial D} dz.\n",
    "$$\n",
    "\n",
    "At a maximum of $D$, $D^*$, this derivative is zero: wlog the terms to be integrated are zero:\n",
    "\n",
    "$$\n",
    "p_R(x) \\frac{1}{D(R(x))} \\frac{\\partial D(R(x))}{\\partial D}  + p_z(z) \\frac{1}{1 - D(G(z))} \\frac{\\partial (1 - D(G(z)))}{\\partial D} = 0.\n",
    "$$\n",
    "\n",
    "Now we can solve for $D^*$, in the following steps:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_R(x) \\frac{1}{D^*(x)} - p_Z(G(z)=x) \\frac{1}{1 - D^*(x)} &= 0 \\\\\n",
    "\\implies \\frac{p_R(x)}{D^*(x)} &= \\frac{p_Z(G(z)=x)}{1 - D^*(x)} \\\\\n",
    "\\implies p_R(x) - D^*(x) p_R(x) &= D^*(x) p_Z(G(z)=x) \\\\\n",
    "\\implies p_R(x) &= D^*(x) (p_R(x) + p_Z(G(z)=x)) \\\\\n",
    "\\implies D^*(x) &= \\frac{p_R(x)}{p_R(x) + p_Z(G(z)=x)} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Abusing notation, we can write $p_Z(G(z)=x) = p_G(x)$, and we have the optimal discriminator:\n",
    "\n",
    "$$\n",
    "D^*(x) = \\frac{p_R(x)}{p_R(x) + p_G(x)}\n",
    "$$\n",
    "\n",
    "QED.\n",
    "\n",
    "Complete the rest of the exercises. You might like to use sympy to help with the algebra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
