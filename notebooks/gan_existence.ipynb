{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Neural Networks for GANs\n",
    "\n",
    "For GANs the neural networks D and G are trained simultaneously. The training process is a minimax game where D tries to maximize the probability of assigning the correct label to the training data and G tries to minimize the probability of D assigning the correct label to the generated data. The training process is repeated until the generator produces data that is indistinguishable from the training data.\n",
    "\n",
    "## Discriminator Loss Optimization\n",
    "\n",
    "We want estimate the best discriminator that minimizes the loss function. This is done by finding the best weight parameters W_D of the D neural network that minimizes the loss function. The discriminator loss function is defined as the cross entropy loss function. The cross entropy loss function is defined as:\n",
    "\n",
    "$$\n",
    " argmin_{W_D} L_D(W_D) = \\sum_{i} ( BCE(D(x_i; W_D), 0) + BCE(D(G(z_i; W_G)), 1) )\n",
    "$$\n",
    "\n",
    "and the binary cross entropy loss function is defined as:\n",
    "\n",
    "$$\n",
    "BCE(p, t) = -t * log(p) - (1 - t) * log(1 - p)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_i$ is the training data\n",
    "- $z_i$ is the noise data\n",
    "- $W_D$ is the weight parameters of the $D$ neural network\n",
    "- $W_G$ is the weight parameters of the $G$ neural network\n",
    "\n",
    "## Generator Loss Optimization\n",
    "\n",
    "We want estimate the best generator that minimizes the loss function. This is done by finding the best weight parameters $W_G$ of the $G$ neural network that minimizes the loss function. The generator loss function is defined as the cross entropy loss function. The cross entropy loss function is defined as:\n",
    "\n",
    "$$\n",
    " argmin_{W_G} L_G(W_G) = \\sum_{i} ( BCE(D(G(z_i; W_G),W_D), 0) ) = \\sum_{i} ( -log(D(G(z_i; W_G), W_D)) )\n",
    "$$\n",
    "\n",
    "or alternatively:\n",
    "\n",
    "$$\n",
    " argmax_{W_G} L_G(W_G) = \\sum_{i} ( log(1 - D(G(z_i; W_G)), W_D) )\n",
    "$$\n",
    "\n",
    "which has been observed to train faster and more stable because the generator loss saturates less often.\n",
    "\n",
    "$(W_G, W_D)$ are the pair of weight parameters of the G and D neural networks.\n",
    "\n",
    "## Optimal Discriminator & Generator\n",
    "\n",
    "We now look for best response optimal  discriminator ( generator) for fixed generator (discriminator). Then we show that application of Brouwer's fixed point theorem to best these best responses, guarantees that there exists a fixed point where the optima are stable.\n",
    "\n",
    "### Optimal Discriminator for Fixed Generator\n",
    "\n",
    "For a given Generator G with fixed $W_G=w_g$, the optimal discriminator is given by optimizing the following loss function:\n",
    "\n",
    "$$\n",
    "Opt_D(W_G) = argmin_{W_D} L_D(W_D, W_G=w_g) = \\sum_{i} ( BCE(D(x_i; W_D), 0) + BCE(D(G(z_i; w_g)), 1) )\n",
    "$$\n",
    "\n",
    "### Optimal Generator for Fixed Discriminator\n",
    "\n",
    "For a given Discriminator $D$ with fixed $W_D=w_d$, the optimal generator is given by optimizing the following loss function:\n",
    "\n",
    "$$\n",
    "Opt_G(W_D) = argmin_{W_G} L_G(W_G,W_D=w_d) = \\sum_{i} ( BCE(D(G(z_i; W_G),W_D=w_d), 0) )\n",
    "$$\n",
    "\n",
    "### Existence of Stable Fixed Point for Generator & Discriminator\n",
    "\n",
    "$$\n",
    "Opt(W_G, W_D) = argmin_{W_D} argmin_{W_G} L_D(W_D, W_G) = \\sum_{i} ( BCE(D(x_i; W_D), 1) + BCE(D(G(z_i; W_G), W_D), 0) )\n",
    "$$\n",
    "\n",
    "is the optimal pair of weight parameters of the $G$ and $D$ neural networks, given the other.\n",
    "\n",
    "Note the domain and range of the functions $OPT_G$ and $OPT_D$\n",
    "\n",
    "$$\n",
    "\\text{OPT}_D: \\mathbb{R}^{\\text{tensor\\_shape}(W_D)} \\rightarrow \\mathbb{R}^{\\text{tensor\\_shape}(W_G)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{OPT}_G: \\mathbb{R}^{\\text{tensor\\_shape}(W_G)} \\rightarrow \\mathbb{R}^{\\text{tensor\\_shape}(W_D)}\n",
    "$$\n",
    "\n",
    "## Brouwer's Fixed Point Theorem\n",
    "\n",
    "Let $f: X \\to X$ be a continuous function on a compact convex set  $X$ in a Euclidean space. Then, there exists at least one point  $x^* \\in X$ such that:\n",
    "\n",
    "$$\n",
    "f(x^*) = x^*\n",
    "$$\n",
    "\n",
    "### Application to GANs\n",
    "\n",
    "$OPT$ is effectively continuous on a compact convex set on the Euclidean space.\n",
    "\n",
    "This means that there exists a fixed point $s^*$ for the function $OPT$.\n",
    "\n",
    "$$\n",
    "OPT(s^*) = s^*\n",
    "$$\n",
    "\n",
    "Let's call that $s^* = (W^{*}_D, W^{*}_G)$.\n",
    "\n",
    "So, there exists a stable point, $(W^{*}_D, W^{*}_G)$, where generator and discriminator return same optima.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Opt}(W_G, W_D) \n",
    "&= \\arg\\min_{W_D} \\arg\\min_{W_G} L_D(W_D, W_G) \\\\\n",
    "&= \\sum_{i} \\Big( \\text{BCE}(D(x_i; W_D), 1) + \\text{BCE}(D(G(z_i; W_G), W_D), 0) \\Big) \\\\\n",
    "&= \\arg\\min_{w_d} \\arg\\min_{W_G} L_D(W_D = w_d, W_G) \\\\\n",
    "&= \\arg\\min_{w_g} \\arg\\min_{W_D} L_D(W_D, W_G=w_g) \\\\\n",
    "&= \\sum_{i} \\Big( \\text{BCE}(D(x_i; w_d), 1) + \\text{BCE}(D(G(z_i; W_G), w_d), 0) \\Big) \\\\\n",
    "&= \\text{opt}_{G}(W^{*}_D) \\\\\n",
    "&= \\text{opt}_{D}(W^{*}_G)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Conclusion: GANs Existence\n",
    "\n",
    "So, if we optimize the loss functions for the generator and discriminator, we will eventually reach a point where the generator produces data that is indistinguishable from the training data for the discriminator.\n",
    "\n",
    "This is another way of saying that there exists a stable point where generator and discriminator can reach a Nash equilibrium.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
