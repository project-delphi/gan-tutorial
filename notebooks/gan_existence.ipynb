{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Neural Networks for GANs\n",
    "\n",
    "For GANs the neural networks D and G are trained simultaneously. The training process is a minimax game where D tries to maximize the probability of assigning the correct label to the training data and G tries to minimize the probability of D assigning the correct label to the generated data. The training process is repeated until the generator produces data that is indistinguishable from the training data.\n",
    "\n",
    "## Discriminator Loss Optimization\n",
    "\n",
    "We want estimate the best discriminator that minimizes the loss function. This is done by finding the best weight parameters W_D of the D neural network that minimizes the loss function. The discriminator loss function is defined as the cross entropy loss function. The cross entropy loss function is defined as:\n",
    "\n",
    "$$\n",
    " argmin_{W_D} L_D(W_D) = \\sum_{i} ( BCE(D(x_i; W_D), 0) + BCE(D(G(z_i; W_G)), 1) )\n",
    "$$\n",
    "\n",
    "and the binary cross entropy loss function is defined as:\n",
    "\n",
    "$$\n",
    "BCE(p, t) = -t * log(p) - (1 - t) * log(1 - p)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_i$ is the training data\n",
    "- $z_i$ is the noise data\n",
    "- $W_D$ is the weight parameters of the $D$ neural network\n",
    "- $W_G$ is the weight parameters of the $G$ neural network\n",
    "\n",
    "## Generator Loss Optimization\n",
    "\n",
    "We want estimate the best generator that minimizes the loss function. This is done by finding the best weight parameters $W_G$ of the $G$ neural network that minimizes the loss function. The generator loss function is defined as the cross entropy loss function. The cross entropy loss function is defined as:\n",
    "\n",
    "$$\n",
    " argmin_{W_G} L_G(W_G) = \\sum_{i} ( BCE(D(G(z_i; W_G)), 0) ) = \\sum_{i} ( -log(D(G(z_i; W_G))) )\n",
    "$$\n",
    "\n",
    "or alternatively:\n",
    "\n",
    "$$\n",
    " argmax_{W_G} L_G(W_G) = \\sum_{i} ( Log(1 - D(G(z_i; W_G))) )\n",
    "$$\n",
    "\n",
    "which has been observed to train faster and more stable because the generator loss saturates less often.\n",
    "\n",
    "$(W_G, W_D)$ are the pair of weight parameters of the G and D neural networks.\n",
    "\n",
    "## Optimal Discriminator & Generator\n",
    "\n",
    "For a given Generator G with fixed $W_G$, the optimal discriminator is given by optimizing the following loss function:\n",
    "\n",
    "$$\n",
    "Opt_D(W_G) = argmin_{W_D} L_D(W_D) = \\sum_{i} ( BCE(D(x_i; W_D), 0) + BCE(D(G(z_i; W_G)), 1) )\n",
    "$$\n",
    "\n",
    "For a given Discriminator D with fixed $W_D$, the optimal generator is given by optimizing the following loss function:\n",
    "\n",
    "$$\n",
    "Opt_G(W_D) = argmin_{W_G} L_G(W_G) = \\sum_{i} ( BCE(D(G(z_i; W_G)), 0) )\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "Opt(W_G, W_D) = (Opt_D(W_G), Opt_G(W_D)) \n",
    "$$\n",
    "\n",
    "is the optimal pair of weight parameters of the G and D neural networks, given the other.\n",
    "\n",
    "Note the domain and range of the functions $OPT_G$ and $OPT_D$\n",
    "\n",
    "$$\n",
    "OPT_D: \\mathbb{R}^{\\text{tensor\\_shape}(W_D)} \\rightarrow \\mathbb{R}^{\\text{tensor\\_shape}(W_G)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "OPT_G: \\mathbb{R}^{\\text{tensor\\_shape}(W_G)} \\rightarrow \\mathbb{R}^{\\text{tensor\\_shape}(W_D)}\n",
    "$$\n",
    "\n",
    "## Brouwer's Fixed Point Theorem\n",
    "\n",
    "Let $f: X \\to X$ be a continuous function on a compact convex set  $X$ in a Euclidean space. Then, there exists at least one point  $x^* \\in X$ such that:\n",
    "\n",
    "$$\n",
    "f(x^*) = x^*\n",
    "$$\n",
    "\n",
    "### Application to GANs\n",
    "\n",
    "$OPT$ is effectively continuous on a compact convex set on the Euclidean space.\n",
    "\n",
    "This means that there exists a fixed point $s$ for the function $OPT$.\n",
    "\n",
    "$$\n",
    "OPT(s) = s\n",
    "$$\n",
    "\n",
    "Let's call that $s = (W^{*}_D, W^{*}_G)$.\n",
    "\n",
    "## Conclusion: GANs Existence\n",
    "\n",
    "This is another way of saying that there exists a stable point where generator and discriminator can reach a Nash equilibrium.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
