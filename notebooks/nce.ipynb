{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Noise Contrastive Estimation\n",
    "\n",
    "## Introduction\n",
    "NCE is a method to estimate the parameters of a model by contrasting the model's predictions with those of a noise distribution. It is used in the context of language modeling, where the model is trained to predict the next word in a sequence of words. The noise distribution is a distribution over words that is used to generate negative examples. The model is trained to assign higher probabilities to the true next word than to the noise words.\n",
    "\n",
    "Noise Contrastive Estimation (NCE) is a method designed to efficiently learn unnormalized probabilistic models, typically in scenarios where calculating the partition function is computationally expensive or intractable. The key idea behind NCE is to avoid explicitly computing the partition function, which normalizes the probability distribution. Instead, it turns the problem into a binary classification task that distinguishes between real data and noise samples.\n",
    "\n",
    "To better understand NCE in relation to the partition function, let’s break down the concepts:\n",
    "\n",
    "1. The Problem with the Partition Function\n",
    "\n",
    "In most probabilistic models, the probability of an observation is given by:\n",
    "￼\n",
    "where is the unnormalized score (the model’s output for ￼) and ￼ is the partition function:\n",
    "￼\n",
    "or, in continuous cases:\n",
    "￼\n",
    "The partition function ￼ ensures that the total probability over all possible outcomes sums to 1. For models with complex or high-dimensional data (e.g., large vocabularies in NLP, complex image datasets), calculating can be computationally prohibitive because it requires summing over or integrating over all possible values of ￼, which is often not feasible. This is where NCE comes in.\n",
    "\n",
    "2. The NCE Approach: Binary Classification\n",
    "\n",
    "NCE addresses the challenge by transforming the learning problem into a classification task, rather than directly estimating the partition function. Instead of modeling the exact distribution ￼, NCE uses a noise distribution ￼, which is easy to sample from and provides a baseline for the classifier.\n",
    "\n",
    "The main idea is to train a model to distinguish between real data samples from the true distribution ￼ and noise samples drawn from the noise distribution ￼. This avoids needing to compute the partition function ￼ directly.\n",
    "\n",
    "3. Formulation of the Problem\n",
    "\n",
    "Suppose we have a dataset where the true distribution is ￼ (e.g., the true distribution of words in a language model). The goal is to learn ￼, but instead of computing it directly, we use the noise distribution ￼, which is easier to handle. For simplicity, assume that ￼ is known, such as a uniform or Gaussian distribution.\n",
    "\n",
    "Binary Classification Task\n",
    "\n",
    "Now, instead of modeling ￼ directly, we model a binary classifier that predicts:\n",
    "\t•\t1 (real) for samples from the true distribution ￼\n",
    "\t•\t0 (fake) for samples from the noise distribution ￼\n",
    "\n",
    "Thus, the model’s task becomes:\n",
    "￼\n",
    "where ￼ is the number of noise samples drawn for each real sample. The classifier now tries to distinguish between samples drawn from the true distribution ￼ and noise samples drawn from ￼.\n",
    "\n",
    "Log-Likelihood Formulation\n",
    "\n",
    "The likelihood for a sample ￼ being classified as “real” can be written as:\n",
    "￼\n",
    "where ￼ is a score function (the output of the model for input ￼) and ￼ is the sigmoid function.\n",
    "\n",
    "The loss function that we optimize during training is the binary cross-entropy between the real data and the noise samples:\n",
    "￼\n",
    "Where:\n",
    "\t•\t￼ is the expectation over the true data distribution ￼\n",
    "\t•\t￼ is the expectation over the noise distribution ￼\n",
    "\n",
    "The loss function aims to make the model assign higher probabilities to real data (samples from ￼) and lower probabilities to noise data (samples from ￼).\n",
    "\n",
    "4. Learning Without the Partition Function\n",
    "\n",
    "In this setup, the partition function ￼ is never explicitly computed. Instead, the model learns the log-ratio ￼ (the unnormalized probability), which is sufficient for learning the distribution ￼.\n",
    "\t•\tKey observation: Since ￼ is only used up to a constant (i.e., ￼), learning the ratio ￼ through binary classification is enough to estimate ￼ without needing the partition function.\n",
    "\n",
    "5. Why NCE Works\n",
    "\n",
    "The success of NCE relies on the following:\n",
    "\t•\tNoise distribution: By using an easily-sampled noise distribution ￼, we can efficiently train the model without needing to normalize over the entire space, which would be required if we directly modeled ￼.\n",
    "\t•\tBinary classification: The model learns to distinguish between data and noise, which simplifies the problem into a supervised classification task with tractable loss and gradient updates.\n",
    "\n",
    "6. Intuition and Efficiency\n",
    "\n",
    "The beauty of NCE lies in its ability to avoid expensive operations (like summing over all possible outcomes for ￼) by:\n",
    "\t•\tReplacing the intractable problem of computing ￼ with a simpler binary classification problem.\n",
    "\t•\tLeveraging the noise distribution ￼, which is easy to handle and does not require the sum over all possible ￼ in the space.\n",
    "\n",
    "Thus, NCE enables the training of models that would otherwise be difficult or impossible to train due to the intractable partition function.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "In summary, NCE sidesteps the need for the partition function ￼ by reframing the problem as a binary classification task that distinguishes between real and noise samples. This approach allows for the efficient learning of unnormalized distributions, making it a valuable technique in scenarios where calculating the partition function is computationally prohibitive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define a simple neural network model for f(x) (un-normalized score)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleNNModel\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m~/Code/github.com/gan-tutorial/.conda/lib/python3.10/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/github.com/gan-tutorial/.conda/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/github.com/gan-tutorial/.conda/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a simple neural network model for f(x) (un-normalized score)\n",
    "class SimpleNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # Flatten MNIST images to vectors\n",
    "        self.fc2 = nn.Linear(128, 1)        # Output a single score\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten the image\n",
    "        x = torch.relu(self.fc1(x))         # Apply ReLU activation\n",
    "        x = self.fc2(x)                     # Output un-normalized score\n",
    "        return x\n",
    "\n",
    "# Define dataset to generate batches of real (MNIST) and fake (noise) data\n",
    "class NCE_MNIST_Dataset(Dataset):\n",
    "    def __init__(self, mnist_data, real_data=True):\n",
    "        self.mnist_data = mnist_data\n",
    "        self.real_data = real_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mnist_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.real_data:\n",
    "            # Sample a real MNIST image\n",
    "            image, label = self.mnist_data[idx]\n",
    "            image = image.unsqueeze(0)  # Add channel dimension (1)\n",
    "            return image, torch.ones(1)  # Label 1 for real data\n",
    "        else:\n",
    "            # Generate random noise data as fake\n",
    "            fake_image = torch.randn(1, 28, 28)  # Random noise (Gaussian)\n",
    "            return fake_image, torch.zeros(1)  # Label 0 for fake data\n",
    "\n",
    "# Initialize MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "model = SimpleNNModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "def train_nce_with_mnist(model, mnist_data, epochs=10, batch_size=64):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Create DataLoader for real data and fake data\n",
    "        real_data_loader = DataLoader(NCE_MNIST_Dataset(mnist_data, real_data=True), batch_size=batch_size, shuffle=True)\n",
    "        noise_data_loader = DataLoader(NCE_MNIST_Dataset(mnist_data, real_data=False), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for (real_images, real_labels), (noise_images, noise_labels) in zip(real_data_loader, noise_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for real and fake data\n",
    "            real_scores = model(real_images)\n",
    "            noise_scores = model(noise_images)\n",
    "\n",
    "            # Compute loss (binary cross entropy)\n",
    "            loss = criterion(real_scores.squeeze(), real_labels.squeeze()) + criterion(noise_scores.squeeze(), noise_labels.squeeze())\n",
    "\n",
    "            # Backpropagate and update parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Train the model using NCE on MNIST\n",
    "train_nce_with_mnist(model, mnist_data, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
