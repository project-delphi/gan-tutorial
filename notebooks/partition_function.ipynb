{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition Function\n",
    "\n",
    "> The normalization constant can be hard to compute as it involves summing or integrating over all possible values, often infeasible in high dimensions or with complex parameters. \n",
    "\n",
    "\n",
    "## Introduction\n",
    "In machine learning we often need to manipulate the partition function of a probability distribution. The partition function is the normalization constant (constant with respect to the values that the random variable can take) that ensures that the probability distribution sums to 1.\n",
    "\n",
    "A specific kind of partition function from statistical physics is the one that appears in the Boltzmann distribution, which is used to model the distribution of particles in a system. This is sometimes called the canonical partition function, and some investigators may not know the other form.\n",
    "\n",
    "# Partition Function vs Normalization Constant\n",
    "\n",
    "In statistics, the terms **partition function** and **normalization constant** are closely related, especially in the context of probability distributions and statistical physics. However, their usage depends on the specific context.\n",
    "\n",
    "## Partition Function\n",
    "The **partition function** is a term often used in **statistical mechanics** and machine learning (e.g., Boltzmann machines). It typically refers to a sum or integral over all possible states of a system, ensuring that probabilities are normalized.\n",
    "\n",
    "Mathematically, for a probability distribution defined by an unnormalized probability $ \\tilde{p}(x) $, the partition function $ Z $ is:\n",
    "\n",
    "- For discrete states:\n",
    "  $$\n",
    "  Z = \\sum_x \\tilde{p}(x)\n",
    "  $$\n",
    "  \n",
    "- For continuous states:\n",
    "  $$\n",
    "  Z = \\int \\tilde{p}(x) \\, dx\n",
    "  $$\n",
    "\n",
    "The partition function is crucial because the normalized probability is:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{\\tilde{p}(x)}{Z}.\n",
    "$$\n",
    "\n",
    "## Normalization Constant\n",
    "The **normalization constant** serves the same purpose: it ensures that a probability distribution integrates or sums to 1. In this sense, the partition function *is* the normalization constant in probabilistic contexts.\n",
    "\n",
    "This term is more commonly used in general probability theory and statistics, where we normalize a function (often a likelihood or posterior) to make it a valid probability density or mass function.\n",
    "\n",
    "## Differences in Context\n",
    "1. **Statistical Physics**:\n",
    "   - The term *partition function* is more common because it connects to physical properties like energy, entropy, and free energy.\n",
    "   - $ Z $ is defined in terms of state energy levels, e.g., \n",
    "     $$\n",
    "     Z = \\sum_x e^{-\\beta E(x)},\n",
    "     $$\n",
    "     where $ E(x) $ is the energy and $ \\beta = 1/kT $ (inverse temperature).\n",
    "\n",
    "2. **Probability and Statistics**:\n",
    "   - The term *normalization constant* is preferred when describing a generic probabilistic model, such as Bayesian posterior normalization.\n",
    "    - The normalization constant ensures that the probability density/mass function integrates or sums to 1.\n",
    "\n",
    "## Formal Definition\n",
    "\n",
    "If $f(x)$ is a probability distribution over a set of values $x$, and $f_0(x)$ is the unnormalized probability distribution, then the partition function is defined as:\n",
    "\n",
    "$$Z = \\sum_{x} f_0(x)$$\n",
    "\n",
    "so $f(x)$ is:\n",
    "\n",
    "$$f(x) = \\frac{f_0(x)}{Z}$$\n",
    "\n",
    "where the sum is over all possible values of $x$.\n",
    "\n",
    "See that Z is constant in the values that $x$ can take, so that the probability distribution sums to 1.\n",
    "\n",
    "### Parametric Models\n",
    "\n",
    "For a pmf characterized by parameters $\\theta$, then the partition function is:\n",
    "\n",
    "$$Z(\\theta) = \\sum_{x} f_0(x; \\theta)$$\n",
    "\n",
    "where the sum is over all possible values of $x$.\n",
    "\n",
    "So $f(x;\\theta)$ is:\n",
    "\n",
    "$$f(x; \\theta) = \\frac{f_0(x; \\theta)}{Z(\\theta)}$$\n",
    "\n",
    "where $f(x; \\theta)$ is the pmf characterized by parameters $\\theta$.\n",
    "\n",
    "Note again that $Z(\\theta)$ is constant in the values that $x$ can take, so that the probability distribution sums to 1. (It's not constant in $\\theta$, which is usually what we vary to fit the model. However for our case, we are not varying $\\theta, so we can consider it constant).\n",
    "\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Discrete Values\n",
    "\n",
    "Let's say we have a probability distribution $f(x)$ over the values $x = \\{1, 2, 3\\}$, and the unnormalized probability distribution is $f_0(x) = \\{2, 3, 1\\}$. The partition function is:\n",
    "\n",
    "$$Z = 2 + 3 + 1 = 6$$\n",
    "\n",
    "Two Examples of Partition Function Calculation:\n",
    "\n",
    "### Energy Based Models\n",
    "\n",
    "In energy based models, the unnormalized probability distribution is defined as:\n",
    "\n",
    "$$f_0(x) = \\exp(-E(x;\\theta))$$\n",
    "\n",
    "where $E(x; \\theta)$ is the energy function. The partition function is then:\n",
    "\n",
    "$$Z = Z(\\theta) = \\sum_{x} \\exp(-E(x; \\theta))$$\n",
    "\n",
    "So that the probability distribution is:\n",
    "\n",
    "$$f(x; \\theta) = \\frac{\\exp(-E(x; \\theta))}{Z(\\theta)}$$\n",
    "\n",
    "### Bayesian Models\n",
    "\n",
    "In Bayesian models, with a prior over the parameter $\\theta$, and observed data D, the partition function is:\n",
    "\n",
    "$$Z = Z(D) = \\int_{\\theta} f(D | \\theta) \\cdot p(\\theta) d\\theta$$\n",
    "\n",
    "where $p(\\theta)$ is the prior over the parameter $\\theta$.\n",
    "\n",
    "See again, that $Z = Z(D)$ is constant in the values that $D$ takes, so that the probability distribution sums to 1.\n",
    "\n",
    "For Bayesian models, $Z(\\theta)$ is also called the marginal likelihood, or evidence.\n",
    "\n",
    "\n",
    "It will normalize the posterior distribution over the parameter $\\theta$ given the data D:\n",
    "\n",
    "$$f(\\theta | D) = \\frac{f(D | \\theta) \\cdot p(\\theta)}{Z(D)}$$\n",
    "\n",
    "where $f(\\theta | D)$ is the posterior distribution over the parameter $\\theta$ given the data D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization constant of Gamma function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\beta^{- \\alpha} \\Gamma\\left(\\alpha\\right)$"
      ],
      "text/plain": [
       "gamma(alpha)/beta**alpha"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols, integrate, exp, gamma\n",
    "\n",
    "# Define variables\n",
    "x, alpha, beta = symbols('x alpha beta', positive=True)\n",
    "\n",
    "# Unnormalized part of the Gamma distribution\n",
    "unnormalized = x**(alpha - 1) * exp(-beta * x)\n",
    "\n",
    "# Partition function Z\n",
    "Z = integrate(unnormalized, (x, 0, float('inf')))\n",
    "\n",
    "# Simplify and compare with the known normalization constant\n",
    "Z_simplified = Z.simplify()\n",
    "Z_simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Work out the normalization constant for the following probability distribution. You can use sympy or work by hand.\n",
    "\n",
    "1. $f(x) = \\frac{1}{2} \\exp(-x^2)$ for $x \\in \\mathbb{R}$.\n",
    "\n",
    "2. beta distribution: $f(x) = \\frac{1}{Z} x^{\\alpha - 1} (1 - x)^{\\beta - 1}$ for $x \\in [0, 1]$, where $Z$ is the normalization constant.\n",
    "\n",
    "3. $f(x) = \\frac{1}{Z} \\exp(-x^2)$ for $x \\in [0, 1]$, where $Z$ is the normalization constant.\n",
    "\n",
    "4. lognormal distribution: $f(x) = \\frac{1}{Z} \\exp(-\\frac{(\\log(x) - \\mu)^2}{2\\sigma^2})$ for $x \\in \\mathbb{R}^+$, where $Z$ is the normalization constant.\n",
    "\n",
    "Do the same but for energy based models:\n",
    "\n",
    "5. $E(x) = x^2$ for $x \\in \\mathbb{R}$.\n",
    "\n",
    "6. $E(x) = x^2$ for $x \\in [0, 1]$.\n",
    "\n",
    "7. $E(x) = \\frac{(\\log(x) - \\mu)^2}{2\\sigma^2}$ for $x \\in \\mathbb{R}^+$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
