{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Loss Functions\n",
    "\n",
    "## Introduction\n",
    "\n",
    "There are a few special kinds of functions when working with GANs. These functions are used to train the generator and the discriminator. In this notebook, we will discuss the following loss functions:\n",
    "\n",
    "1. Discriminator Loss\n",
    "2. Generator Loss\n",
    "3. Binary Cross Entropy Loss\n",
    "4. Wasserstein Loss\n",
    "5. KL Divergence Loss\n",
    "6. JSD Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Loss in Terms of Cross-Entropy Loss\n",
    "\n",
    "### Discriminator Loss\n",
    "\n",
    "The discriminator $D(r)$ outputs the probability that $r$ is real (i.e., sampled from $p_r$).\n",
    "\n",
    "- For real samples $r \\sim p_{r}$, the true label is $1$, and the discriminator wants to maximize $\\log D(r)$.\n",
    "\n",
    "- For fake samples $f \\sim P_{F}$, the true label is $0$, and the discriminator wants to maximize $\\log(1 - D(f))$.\n",
    "\n",
    "The discriminator's objective can be written as:\n",
    "$$\n",
    "L_D = - \\mathbb{E}_{R} [\\log D(R)] - \\mathbb{E}_{Z} [\\log(1 - D(G(Z)))]\n",
    "$$\n",
    "\n",
    "This is equivalent to the binary cross-entropy loss for the discriminator:\n",
    "$$\n",
    "L_D = \\text{BCE}(D(R), 1) + \\text{BCE}(D(G(Z)), 0)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\text{BCE}(p, y) = -[y \\log p + (1-y) \\log (1-p)]\n",
    "$$\n",
    "\n",
    "is the binary cross-entropy loss, and:\n",
    "- $D(R)$: The discriminator's prediction for real data.\n",
    "- $D(G(Z))$: The discriminator's prediction for generated data.\n",
    "\n",
    "## Generator Loss\n",
    "\n",
    "The generator $G(Z)$ aims to produce samples that the discriminator classifies as real. For generated samples $f$, the generator's objective is to maximize $\\log D(G(Z))$, which is equivalent to minimizing:\n",
    "$$\n",
    "L_G = - \\mathbb{E}_{Z} [\\log D(G(Z))]\n",
    "$$\n",
    "\n",
    "This is also a binary cross-entropy loss, where the true label for generated samples is $1$ (i.e., the generator wants the discriminator to classify fake samples as real):\n",
    "$$\n",
    "L_G = \\text{BCE}(D(G(Z)), 1)\n",
    "$$\n",
    "\n",
    "\n",
    "## Alternate Formulation for Generator Loss\n",
    "\n",
    "In some GAN formulations, the generator minimizes $\\log(1 - D(G(Z)))$ instead of maximizing  $\\log D(G(Z))$, leading to:\n",
    "$$\n",
    "L_G' = - \\mathbb{E}_{Z} [\\log(1 - D(G(Z)))]\n",
    "$$\n",
    "\n",
    "This corresponds to the generator trying to \"trick\" the discriminator indirectly, but it has weaker gradients when $D(G(Z))$ is close to $0$. Therefore, the $\\log D(G(Z))$ version is more commonly used for stable training.\n",
    "\n",
    "## Connection to Cross-Entropy\n",
    "\n",
    "Ultimately, both the discriminator and generator losses in GANs are defined using binary cross-entropy loss. The difference is in how the true labels are assigned:\n",
    "- For the discriminator: $1$ for real samples, $0$ for fake samples.\n",
    "- For the generator: $1$ for generated samples (as it tries to fool the discriminator).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
