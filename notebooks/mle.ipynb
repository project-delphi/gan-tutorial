{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE: Cross Entropy and KL Divergence Minimzation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Given a dataset $\\{x_1, x_2, \\dots, x_n\\}$ drawn from an unknown distribution, the goal of MLE is to estimate the parameters $\\theta$ of a probability distribution $p(x|\\theta)$ such that the likelihood of the observed data is maximized.\n",
    "\n",
    "The likelihood function is the joint probability of the observed data:\n",
    "\n",
    "$$\n",
    "L(\\theta) = p(x_1, x_2, \\dots, x_n | \\theta) = \\prod_{i=1}^{n} p(x_i | \\theta)\n",
    "$$\n",
    "\n",
    "The **log-likelihood** is then:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\sum_{i=1}^{n} \\log p(x_i | \\theta)\n",
    "$$\n",
    "\n",
    "MLE finds $\\theta$ by maximizing the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\arg\\max_{\\theta} \\log L(\\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimzing CE Loss & KL Divergence against data density estimate\n",
    "\n",
    "In this notebook, we will show that minimizing the Cross Entropy (CE) loss and the Kullback-Leibler (KL) Divergence between the data distribution and the model distribution are equivalent to maximizing the log-likelihood of the data.\n",
    "\n",
    "Let's consider a dataset $\\{x_1, x_2, \\dots, x_n\\}$ drawn from an unknown distribution $p_{\\text{data}}(x)$.\n",
    "\n",
    "We want to estimate the parameters $\\theta$ of a model distribution $p_{\\text{model}}(x|\\theta)$ such that the model distribution is as close as possible to the data distribution.\n",
    "\n",
    "The data distribution is the crude estimate of the true distribution of the data. The model distribution is the distribution we are trying to learn.\n",
    "\n",
    "For the dataset, the data density estimate is:\n",
    "\n",
    "$$\n",
    "\\hat{p}_{\\text{data}}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\delta(x - x_i)\n",
    "$$\n",
    "\n",
    "where $\\delta(x)$ is the Dirac delta function.\n",
    "\n",
    "The CE loss between the data distribution and the model distribution is:\n",
    "\n",
    "$$\n",
    "\\text{CE}(\\theta) = - \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\left[ \\log p_{\\text{model}}(x|\\theta) \\right]\n",
    "$$\n",
    "\n",
    "For fixed $\\hat{p}_{\\text{data}}(x)$, the CE loss is minimized when the model distribution is as close as possible to the data distribution.\n",
    "\n",
    "Proof:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{CE}(\\theta) &= - \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\left[ \\log p_{\\text{model}}(x|\\theta) \\right] \\\\\n",
    "&= - \\int p_{\\text{data}}(x) \\log p_{\\text{model}}(x|\\theta) dx \\\\\n",
    "&= - \\int p_{\\text{data}}(x) \\log \\left( \\frac{p_{\\text{model}}(x|\\theta)}{p_{\\text{data}}(x)} p_{\\text{data}}(x) \\right) dx \\\\\n",
    "&= - \\int p_{\\text{data}}(x) \\log \\left( \\frac{p_{\\text{model}}(x|\\theta)}{p_{\\text{data}}(x)} \\right) dx - \\int p_{\\text{data}}(x) \\log p_{\\text{data}}(x) dx \\\\\n",
    "&= - \\int p_{\\text{data}}(x) \\log \\left( \\frac{p_{\\text{model}}(x|\\theta)}{p_{\\text{data}}(x)} \\right) dx + \\text{H}(\\hat{p}_{\\text{data}}(x)) \\\\\n",
    "&= \\text{KL}(\\hat{p}_{\\text{data}}(x) || p_{\\text{model}}(x|\\theta)) +\\text{H}(\\hat{p}_{\\text{data}}(x))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\text{H}(\\hat{p}_{\\text{data}}(x))$ is the entropy of the data distribution.\n",
    "\n",
    "This is the sum of the KL Divergence between the data distribution and the model distribution and the entropy of the data distribution.\n",
    "\n",
    "If we substitute the data density estimate $\\hat{p}_{\\text{data}}(x)$ with the true data distribution $p_{\\text{data}}(x)$, then:\n",
    "\n",
    "$$\n",
    "\\text{CE}(\\theta) = \\text{KL}(p_{\\text{data}}(x) || p_{\\text{model}}(x|\\theta)) - \\text{H}(p_{\\text{data}}(x))\n",
    "$$\n",
    "\n",
    "Ignoring the entropy term, the CE loss is minimized when the KL Divergence between the data distribution and the model distribution is minimized.\n",
    "\n",
    "Substituting $\\hat{p}_{\\text{data}}(x)$ with the true data distribution $p_{\\text{data}}(x)$, and using the dirac delta function, the CE loss becomes:\n",
    "\n",
    "$$\n",
    "\\text{CE}(\\theta) = - \\frac{1}{n} \\sum_{i=1}^{n} \\log p_{\\text{model}}(x_i|\\theta)\n",
    "$$\n",
    "\n",
    "because $p_{\\text{data}}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\delta(x - x_i)$ which gives:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{CE}(\\theta) &= - \\int p_{\\text{data}}(x) \\log p_{\\text{model}}(x|\\theta) dx \\\\\n",
    "&= - \\int \\frac{1}{n} \\sum_{i=1}^{n} \\delta(x - x_i) \\log p_{\\text{model}}(x|\\theta) dx \\\\\n",
    "&= - \\frac{1}{n} \\sum_{i=1}^{n} \\log p_{\\text{model}}(x_i|\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "So when we minimize the CE loss, we are maximizing the log-likelihood of the data, $\\log L(\\theta)$.\n",
    "\n",
    "At the minimum of the CE loss, $\\hat{\\theta}$, CE($\\hat{\\theta}$) ( a constant away from $l = log(L(\\hat{\\theta})$)) the model distribution is as close as possible to the data distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
