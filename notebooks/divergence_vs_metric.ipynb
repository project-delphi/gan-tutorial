{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergence vs Metric Functions\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will discuss the difference between divergence and metric functions. We will also see how they are related to each other.\n",
    "\n",
    "## Divergence\n",
    "\n",
    "Divergence is a measure of how different two probability distributions are from each other. It is a non-negative scalar value that quantifies the difference between two distributions. Divergence is often used in machine learning and statistics to compare the similarity between two probability distributions.\n",
    "\n",
    "There are many different divergence functions, such as Kullback-Leibler divergence, cross entropy divergence. Each of these divergence functions has its own properties and applications.\n",
    "\n",
    "Divergence is not always well defined. It depends on context and community. For example:\n",
    "\n",
    "- In statistics, divergence is often used to measure the difference between two probability distributions.\n",
    "\n",
    "- In physics, divergence is used to measure the flow of a vector field.\n",
    "\n",
    "- In mathematics, divergence is used to measure the rate at which a vector field spreads out from a point.\n",
    "\n",
    "All metric functions are divergence functions, but not all divergence functions are metric functions.\n",
    "\n",
    "For example:\n",
    "\n",
    "Jensen-Shannon divergence, and Hellinger distance are all divergence functions that are also metric functions. It's a question of context and community.\n",
    "\n",
    "## Metric Function\n",
    "\n",
    "A metric function is a function that takes two points as input and outputs a scalar value. The metric function is non-negative and is equal to zero if and only if the two points are the same. The metric function satisfies the following properties:\n",
    "\n",
    "1. Non-negativity: $d(x, y) \\geq 0$ for all $x, y \\in X$.\n",
    "\n",
    "2. Identity of indiscernibles: $d(x, y) = 0$ if and only if $x = y$.\n",
    "\n",
    "3. Symmetry: $d(x, y) = d(y, x)$ for all $x, y \\in X$.\n",
    "\n",
    "4. Triangle inequality: $d(x, y) + d(y, z) \\geq d(x, z)$ for all $x, y, z \\in X$.\n",
    "\n",
    "## JS Divergence is a Metric Function\n",
    "\n",
    "The Jensen-Shannon divergence is a divergence function that is also a metric function. The Jensen-Shannon divergence is defined as:\n",
    "\n",
    "$JSD(P, Q) = \\frac{1}{2} KL(P || M) + \\frac{1}{2} KL(Q || M)$\n",
    "\n",
    "where $KL(P || Q)$ is the Kullback-Leibler divergence between two probability distributions $P$ and $Q$, and $M = \\frac{1}{2}(P + Q)$ is the average of the two distributions.\n",
    "\n",
    "Because it is a metric, it offers some advantages over other divergence functions. For example it is defined when the **model distribution q is zero for some values of x, which is not the case for the KL divergence**.\n",
    "\n",
    "\n",
    "## Wasserstein Distance\n",
    "\n",
    "The Wasserstein distance is another metric function that is used to measure the difference between two probability distributions. The Wasserstein distance is defined as:\n",
    "\n",
    "$W(P, Q) = \\inf_{\\Pi(P, Q)} \\int c(x, y) d\\Pi(x, y)$\n",
    "\n",
    "where $\\Pi(P, Q)$ is the set of all joint probability distributions \n",
    "with marginals $P$, $Q$ and:\n",
    "\n",
    "* $c(x, y)$ is the cost of transporting a unit of mass from $x$ to $y$.\n",
    "\n",
    "### Wasserstein Loss Formula for GANs\n",
    "\n",
    "In the context of Generative Adversarial Networks (GANs), the Wasserstein distance is used as a loss function to measure the difference between the generated and real data distributions. The Wasserstein loss function is defined as:\n",
    "\n",
    "$W(P_r, P_g) = \\inf_{\\gamma \\in \\Pi(P_r, P_g)} \\mathbb{E}_{(x, y) \\sim \\gamma} [||x - y||]$\n",
    "\n",
    "where $P_r$ is the real data distribution, $P_g$ is the generated data distribution, and $\\Pi(P_r, P_g)$ is the set of all joint probability distributions with marginals $P_r$ and $P_g$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein distance: 0.3999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def wasserstein_distance(P, Q):\n",
    "    \"\"\"\n",
    "    Compute the Wasserstein distance (1D) between two distributions P and Q.\n",
    "\n",
    "    Parameters:\n",
    "    P (ndarray): 1D array of probabilities or histogram bins for distribution P.\n",
    "    Q (ndarray): 1D array of probabilities or histogram bins for distribution Q.\n",
    "\n",
    "    Returns:\n",
    "    float: The Wasserstein distance between P and Q.\n",
    "    \"\"\"\n",
    "    # Normalize the histograms (make sure they sum to 1)\n",
    "    P = P / np.sum(P)\n",
    "    Q = Q / np.sum(Q)\n",
    "    \n",
    "    # Compute cumulative distribution functions (CDF)\n",
    "    cdf_P = np.cumsum(P)\n",
    "    cdf_Q = np.cumsum(Q)\n",
    "    \n",
    "    # Compute Wasserstein distance (L1 distance between the CDFs)\n",
    "    return np.sum(np.abs(cdf_P - cdf_Q))\n",
    "\n",
    "# Example distributions (histograms)\n",
    "P = np.array([0.1, 0.3, 0.4, 0.2])\n",
    "Q = np.array([0.2, 0.1, 0.3, 0.4])\n",
    "\n",
    "# Compute Wasserstein distance\n",
    "distance = wasserstein_distance(P, Q)\n",
    "print(\"Wasserstein distance:\", distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of using Wasserstein distance for GANs:\n",
    "\n",
    "In particular, compared to KL divergence for GANs, Wasserstein distance is more stable and provides better gradients for training the generator.\n",
    "\n",
    " Why is this? Because the Wasserstein distance is a metric function, it satisfies the triangle inequality, which makes it easier to optimize. \n",
    " \n",
    " Why? Because the triangle inequality ensures that the distance between two points is always less than or equal to the sum of the distances between those points and a third point. This property makes the Wasserstein distance more stable and easier to optimize than other divergence functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 0.07279188207395282\n",
      "JS Distance: 0.13794915308916347\n",
      "Wasserstein Distance: 0.052222296152880265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_probs = nb_model.predict_proba(X_test)\n",
    "\n",
    "# Fit Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=200)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_probs = lr_model.predict_proba(X_test)\n",
    "\n",
    "# Calculate KL divergence\n",
    "kl_divergence = np.sum(nb_probs * np.log(nb_probs / lr_probs), axis=1).mean()\n",
    "\n",
    "# Calculate JS distance\n",
    "js_distance = jensenshannon(nb_probs, lr_probs, axis=1).mean()\n",
    "\n",
    "# Calculate Wasserstein distance\n",
    "wasserstein_dist = np.mean([wasserstein_distance(nb_probs[i], lr_probs[i]) for i in range(len(nb_probs))])\n",
    "\n",
    "print(f\"KL Divergence: {kl_divergence}\")\n",
    "print(f\"JS Distance: {js_distance}\")\n",
    "print(f\"Wasserstein Distance: {wasserstein_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
