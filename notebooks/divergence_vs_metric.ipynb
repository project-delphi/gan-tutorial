{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergence vs Metric Functions\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will discuss the difference between divergence and metric functions. We will also see how they are related to each other.\n",
    "\n",
    "## Divergence\n",
    "\n",
    "Divergence is a measure of how different two probability distributions are from each other. It is a non-negative scalar value that quantifies the difference between two distributions. Divergence is often used in machine learning and statistics to compare the similarity between two probability distributions.\n",
    "\n",
    "There are many different divergence functions, such as Kullback-Leibler divergence, cross entropy divergence. Each of these divergence functions has its own properties and applications.\n",
    "\n",
    "Divergence is not always well defined. It depends on context and community. For example:\n",
    "\n",
    "- In statistics, divergence is often used to measure the difference between two probability distributions.\n",
    "\n",
    "- In physics, divergence is used to measure the flow of a vector field.\n",
    "\n",
    "- In mathematics, divergence is used to measure the rate at which a vector field spreads out from a point.\n",
    "\n",
    "All metric functions are divergence functions, but not all divergence functions are metric functions.\n",
    "\n",
    "For example:\n",
    "\n",
    "Jensen-Shannon divergence, and Hellinger distance are all divergence functions that are also metric functions. It's a question of context and community.\n",
    "\n",
    "## Metric Function\n",
    "\n",
    "A metric function is a function that takes two points as input and outputs a scalar value. The metric function is non-negative and is equal to zero if and only if the two points are the same. The metric function satisfies the following properties:\n",
    "\n",
    "1. Non-negativity: $d(x, y) \\geq 0$ for all $x, y \\in X$.\n",
    "\n",
    "2. Identity of indiscernibles: $d(x, y) = 0$ if and only if $x = y$.\n",
    "\n",
    "3. Symmetry: $d(x, y) = d(y, x)$ for all $x, y \\in X$.\n",
    "\n",
    "4. Triangle inequality: $d(x, y) + d(y, z) \\geq d(x, z)$ for all $x, y, z \\in X$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 0.24494897427831774\n",
      "Manhattan Distance: 0.3999999999999999\n",
      "Cosine Distance: 0.050751810770051864\n",
      "KL Divergence: 0.08512282595722162\n",
      "JS Divergence: 0.14799046918127484\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean, cityblock, cosine\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# Example data\n",
    "p = [0.1, 0.2, 0.7]\n",
    "q = [0.2, 0.3, 0.5]\n",
    "\n",
    "# Metric functions\n",
    "euclidean_distance = euclidean(p, q)\n",
    "manhattan_distance = cityblock(p, q)\n",
    "cosine_distance = cosine(p, q)\n",
    "\n",
    "# Divergence functions\n",
    "kl_divergence = entropy(p, q)\n",
    "js_divergence = jensenshannon(p, q)\n",
    "\n",
    "print(f\"Euclidean Distance: {euclidean_distance}\")\n",
    "print(f\"Manhattan Distance: {manhattan_distance}\")\n",
    "print(f\"Cosine Distance: {cosine_distance}\")\n",
    "print(f\"KL Divergence: {kl_divergence}\")\n",
    "print(f\"JS Divergence: {js_divergence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 0.07279188207395282\n",
      "JS Distance: 0.13794915308916347\n",
      "Wasserstein Distance: 0.052222296152880265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_probs = nb_model.predict_proba(X_test)\n",
    "\n",
    "# Fit Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=200)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_probs = lr_model.predict_proba(X_test)\n",
    "\n",
    "# Calculate KL divergence\n",
    "kl_divergence = np.sum(nb_probs * np.log(nb_probs / lr_probs), axis=1).mean()\n",
    "\n",
    "# Calculate JS distance\n",
    "js_distance = jensenshannon(nb_probs, lr_probs, axis=1).mean()\n",
    "\n",
    "# Calculate Wasserstein distance\n",
    "wasserstein_dist = np.mean([wasserstein_distance(nb_probs[i], lr_probs[i]) for i in range(len(nb_probs))])\n",
    "\n",
    "print(f\"KL Divergence: {kl_divergence}\")\n",
    "print(f\"JS Distance: {js_distance}\")\n",
    "print(f\"Wasserstein Distance: {wasserstein_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
