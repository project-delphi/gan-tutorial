{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback-Leibler (KL) Divergence Loss\n",
    "\n",
    "## Definition\n",
    "\n",
    "The **Kullback-Leibler (KL) divergence** is a measure of how one probability distribution diverges from a second, expected probability distribution. It quantifies the \"distance\" between two distributions, though it is not technically a true distance since it is not symmetric. The formula for KL divergence between two distributions \\( P \\) and \\( Q \\) is:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) = \\sum_{i} P(x_i) \\log \\frac{P(x_i)}{Q(x_i)}\n",
    "$$\n",
    "\n",
    "For continuous distributions, this becomes:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( P(x) \\) is the true distribution (often the target or ground truth).\n",
    "- \\( Q(x) \\) is the approximate distribution (usually the predicted distribution).\n",
    "- The KL divergence tells us how much information is lost when using \\( Q \\) to approximate \\( P \\).\n",
    "\n",
    "## Intuitive Explanation\n",
    "\n",
    "KL divergence can be thought of as the **extra surprise** or **extra information** that we gain when using \\( Q \\) to represent \\( P \\), instead of using \\( P \\) directly. If the two distributions are identical, then \\( D_{\\text{KL}}(P \\parallel Q) = 0 \\), meaning there's no \"extra surprise.\" If they differ, the divergence will be greater, indicating more error or inefficiency in the approximation.\n",
    "\n",
    "In machine learning, we minimize the KL divergence to make the predicted distribution \\( Q \\) as close as possible to the true distribution \\( P \\). This loss encourages the model to adjust its predictions to more closely match the actual data distribution.\n",
    "\n",
    "## Link to Cross-Entropy Loss\n",
    "\n",
    "The **cross-entropy loss** is closely related to KL divergence. In fact, when we have a true distribution \\( P \\) and a predicted distribution \\( Q \\), the cross-entropy loss between them is defined as:\n",
    "\n",
    "$$\n",
    "H(P, Q) = - \\sum_{i} P(x_i) \\log Q(x_i)\n",
    "$$\n",
    "\n",
    "We can express the cross-entropy loss in terms of KL divergence as follows:\n",
    "\n",
    "$$\n",
    "H(P, Q) = D_{\\text{KL}}(P \\parallel Q) + H(P)\n",
    "$$\n",
    "\n",
    "Where \\( H(P) \\) is the entropy of the true distribution \\( P \\), representing the uncertainty of \\( P \\). Thus, minimizing cross-entropy loss is equivalent to minimizing the KL divergence plus the entropy of the true distribution, which is a constant with respect to the predictions. Therefore, minimizing cross-entropy effectively reduces KL divergence.\n",
    "\n",
    "In classification problems, **cross-entropy** is commonly used as the loss function because it directly relates to the likelihood of the model's predictions, while KL divergence provides more of a measure of distance between two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bernoulli cross entropy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "KL Divergence: 0.020135513550688863\n",
      "Entropy of P: 0.6730116670092565\n",
      "Cross-Entropy: 0.6931471805599453\n",
      "Cross-entropy is equal to KL divergence plus entropy of P\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two categorical distributions P and Q\n",
    "P = np.array([0.4, 0.6])\n",
    "Q = np.array([0.5, 0.5])\n",
    "\n",
    "# Calculate KL divergence\n",
    "kl_divergence = np.sum(P * np.log(P / Q))\n",
    "print(f\"KL Divergence: {kl_divergence}\")\n",
    "\n",
    "# Calculate entropy of P\n",
    "entropy_P = -np.sum(P * np.log(P))\n",
    "print(f\"Entropy of P: {entropy_P}\")\n",
    "\n",
    "# Calculate cross-entropy between P and Q\n",
    "cross_entropy = -np.sum(P * np.log(Q))\n",
    "print(f\"Cross-Entropy: {cross_entropy}\")\n",
    "\n",
    "# Verify that cross-entropy is equal to KL divergence plus entropy of P\n",
    "assert np.isclose(cross_entropy, kl_divergence + entropy_P), \"Cross-entropy is not equal to KL divergence plus entropy of P\"\n",
    "print(\"Cross-entropy is equal to KL divergence plus entropy of P\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL Divergence and Cross-Entropy for Normal Distributions\n",
    "\n",
    "Let's consider two normal distributions \\( P \\) and \\( Q \\) with means \\( \\mu_P \\), \\( \\mu_Q \\) and standard deviations \\( \\sigma_P \\), \\( \\sigma_Q \\).\n",
    "\n",
    "If we try to get the entropy using sympy, we get close:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the normal distribution: Piecewise((nan, Eq((-2*log(exp(-mu**2/(2*sigma**2))/sigma) + log(2*pi))*exp(-mu**2/(2*sigma**2))/sigma, 0)), (oo*(1 - log(exp(-mu**2/(2*sigma**2))/sigma))*exp(-mu**2/(2*sigma**2))/(sigma*Abs((2*log(exp(-mu**2/(2*sigma**2))/sigma) - log(2*pi))/sigma)), True))\n"
     ]
    }
   ],
   "source": [
    "# Define symbols\n",
    "x, mu, sigma = sp.symbols('x mu sigma')\n",
    "\n",
    "# Define the pdf of the normal distribution\n",
    "pdf = (1 / (sigma * sp.sqrt(2 * sp.pi))) * sp.exp(-((x - mu)**2) / (2 * sigma**2))\n",
    "\n",
    "# Define the entropy integral\n",
    "entropy_integral = -sp.integrate(pdf * sp.log(pdf), (x, -sp.oo, sp.oo))\n",
    "\n",
    "# Simplify the result\n",
    "entropy_simplified = sp.simplify(entropy_integral)\n",
    "print(f\"Entropy of the normal distribution: {entropy_simplified}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of a normal distribution can be verified to be:\n",
    "\n",
    "$$\n",
    "H(P) = \\frac{1}{2} \\log(2 \\pi e \\sigma_P^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL divergence between two normal distributions is given by:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) = \\log \\frac{\\sigma_Q}{\\sigma_P} + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2} - \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "We will use `sympy` to calculate the KL divergence and show that it is equal to the cross-entropy plus the entropy of \\( P \\).\n",
    "\n",
    "\n",
    "In this example, we have used `sympy` to symbolically calculate the KL divergence, entropy, and cross-entropy for two normal distributions. The calculations confirm that the cross-entropy is equal to the KL divergence plus the entropy of \\( P \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: (sigma_P**2 + sigma_Q**2*(2*log(sigma_Q/sigma_P) - 1) + (mu_P - mu_Q)**2)/(2*sigma_Q**2)\n",
      "Entropy of P: log(sqrt(2)*sqrt(pi)*sigma_P*exp(1/2))\n",
      "Cross-Entropy: (sigma_P**2 + sigma_Q**2*(2*log(sigma_P) + 2*log(sigma_Q/sigma_P) + log(2) + log(pi)) + (mu_P - mu_Q)**2)/(2*sigma_Q**2)\n",
      "Cross-entropy is equal to KL divergence plus entropy of P\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "mu_P, mu_Q, sigma_P, sigma_Q = sp.symbols('mu_P mu_Q sigma_P sigma_Q')\n",
    "\n",
    "# KL divergence formula for normal distributions\n",
    "kl_divergence = sp.log(sigma_Q / sigma_P) + (sigma_P**2 + (mu_P - mu_Q)**2) / (2 * sigma_Q**2) - sp.Rational(1, 2)\n",
    "kl_divergence_simplified = sp.simplify(kl_divergence)\n",
    "print(f\"KL Divergence: {kl_divergence_simplified}\")\n",
    "\n",
    "# Entropy of P\n",
    "entropy_P = sp.log(sigma_P * sp.sqrt(2 * sp.pi * sp.exp(1)))\n",
    "print(f\"Entropy of P: {entropy_P}\")\n",
    "\n",
    "# Cross-entropy between P and Q\n",
    "cross_entropy = entropy_P + kl_divergence\n",
    "cross_entropy_simplified = sp.simplify(cross_entropy)\n",
    "print(f\"Cross-Entropy: {cross_entropy_simplified}\")\n",
    "\n",
    "# Verify that cross-entropy is equal to KL divergence plus entropy of P\n",
    "assert sp.simplify(cross_entropy - (kl_divergence + entropy_P)) == 0, \"Cross-entropy is not equal to KL divergence plus entropy of P\"\n",
    "print(\"Cross-entropy is equal to KL divergence plus entropy of P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
